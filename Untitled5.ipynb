{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtA8Aw3UkuYZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "pd.options.display.float_format = '{:.3f}'.format\n",
        "import math\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(\"//content/breast-cancer.csv\")\n",
        "df.info()\n",
        "df.describe().T\n",
        "df.columns\n",
        "\n",
        "data.drop(columns=['id'], inplace=True)\n",
        "X = data.drop(columns=['diagnosis'])\n",
        "y = data['diagnosis'].map({'M': 1, 'B': 0})\n",
        "\n",
        "def split(df,label):\n",
        "    X_tr, X_te, Y_tr, Y_te = train_test_split(df, label, test_size=0.25, random_state=42)\n",
        "    return X_tr, X_te, Y_tr, Y_te\n",
        "\n",
        "# Normalization\n",
        "\n",
        "scaler = Normalizer()\n",
        "\n",
        "for i in [df.columns]:\n",
        "    df[i] = scaler.fit_transform(df[i])\n",
        "\n",
        "dataـbc=split(x,y)\n"
      ],
      "metadata": {
        "id": "3rjbp6nUlQVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#corrolation\n",
        "\n",
        "def correlation(dataset, cor):\n",
        "    dataـbc = dataset.copy()\n",
        "    col_corr = set()\n",
        "    corr_matrix = dataset.corr()\n",
        "    for i in range(len(corr_matrix.columns)):\n",
        "        for j in range(i):\n",
        "            if abs(corr_matrix.iloc[i, j]) > cor:\n",
        "                colname = corr_matrix.columns[i]\n",
        "                col_corr.add(colname)\n",
        "    df.drop(col_corr,axis = 1,inplace = True)\n",
        "    return dataـbc\n",
        "\n",
        "correlation(x,0.9).shape\n"
      ],
      "metadata": {
        "id": "gHpSrmVDlfPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#variance\n",
        "\n",
        "def variance_threshold(dataـbc,th):\n",
        "    var_thres=VarianceThreshold(threshold=th)\n",
        "    var_thres.fit(dataـbc)\n",
        "    new_cols = var_thres.get_support()\n",
        "    return dataـbc.iloc[:,new_cols]\n"
      ],
      "metadata": {
        "id": "c30G2iS-libe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataـbc2=variance_threshold(x,0.1)\n",
        "dataـbc 2.shape\n"
      ],
      "metadata": {
        "id": "MWIOGtA9llMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MI\n",
        "\n",
        "mi_scores = mutual_info_classif(X_scaled, y)\n",
        "selector = SelectKBest(score_func=mutual_info_classif, k=10)\n",
        "selector.fit(X_scaled, y)\n",
        "selected_features = X.columns[selector.get_support()]\n",
        "mi_df = pd.DataFrame({'Feature': X.columns, 'Mutual Information Score': mi_scores})\n",
        "mi_df['Selected'] = mi_ dataـbc ['Feature'].apply(lambda x: 'Selected' if x in selected_features else 'Not Selected')\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x='Mutual Information Score', y='Feature', hue='Selected', data= mi_dataـbc, palette={'Selected': 'green', 'Not Selected': 'gray'})\n",
        "plt.title('Mutual Information Scores for Breast Cancer Features (Top Features Highlighted)')\n",
        "plt.xlabel('Mutual Information Score')\n",
        "plt.ylabel('Feature')\n",
        "plt.legend(title='Feature Selection')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FCioIXtMloCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#forward feture selection\n",
        "\n",
        "clf = ExtraTreesClassifier(n_estimators=60, n_jobs=-1,random_state=42)\n",
        "\n",
        " sfs1 = SequentialFeatureSelector(clf,\n",
        " k_features=10,\n",
        " forward=True,\n",
        " floating=False,\n",
        " verbose=2,\n",
        " scoring='accuracy',\n",
        " cv=9)\n",
        "\n",
        "sfs1 = SequentialFeatureSelector(\n",
        "    clf, n_features_to_select=9, direction=\"forward\"\n",
        ")\n",
        "sfs1 = sfs1.fit(x, y)\n"
      ],
      "metadata": {
        "id": "JkdqkGqblrEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# backward feature selection\n",
        "\n",
        "def backward_elimination_kmeans(X, y, threshold=0.1):\n",
        "    features = X.columns.tolist()\n",
        "    while len(features) > 0:\n",
        "        kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "        kmeans.fit(X[features])\n",
        "        silhouette_avg = silhouette_score(X[features], kmeans.labels_)\n",
        "        print(f\"feature: {features}, silhouette score: {silhouette_avg:.4f}\")\n",
        "        scores = []\n",
        "        for feature in features:\n",
        "            temp_features = [f for f in features if f != feature]\n",
        "            kmeans_temp = KMeans(n_clusters=2, random_state=42)\n",
        "            kmeans_temp.fit(X[temp_features])\n",
        "            score = silhouette_score(X[temp_features], kmeans_temp.labels_)\n",
        "            scores.append((feature, score))\n",
        "        worst_feature, worst_score = min(scores, key=lambda x: x[1])\n",
        "        if worst_score < silhouette_avg - threshold:\n",
        "            features.remove(worst_feature)\n",
        "            print(f\"feature '{worst_feature}' deleted\")\n",
        "        else:\n",
        "            break\n",
        "    return features\n",
        "selected_features = backward_elimination_kmeans(X, y)\n",
        "print(selected_features)\n"
      ],
      "metadata": {
        "id": "tVUbgzzzluD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#comprehensive feature selection\n",
        "\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "efs = ExhaustiveFeatureSelector(model,\n",
        "                                min_features=1,\n",
        "                                max_features=10,\n",
        "                                scoring='accuracy',\n",
        "                                print_progress=True)\n",
        "\n",
        "efs = efs.fit(X_train.values, y_train.values)\n",
        "\n",
        "selected_features = X.columns[list(efs.best_idx_)]\n",
        "print(\"selected features\", selected_features.tolist())\n",
        "\n",
        "print(\"model accuracy selected features:\", efs.best_score_)\n",
        "\n"
      ],
      "metadata": {
        "id": "WW5vnwDTlxFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#regularization(ridg & lasso)\n",
        "\n",
        "classifiers = ['Linear Reg',\n",
        "               'Ridge a = 0.001','Ridge a = 0.01','Ridge a = 0.1',\n",
        "               'Lasso a = 0.001','Lasso a = 0.01','Lasso a = 0.1']\n",
        "models = [LinearRegression(),\n",
        "          Ridge(alpha = 0.001),Ridge(alpha = 0.01),Ridge(alpha = 0.1),\n",
        "          Lasso(alpha = 0.001),Lasso(alpha = 0.01),Lasso(alpha = 0.1)]\n",
        "KFold_Score = pd.DataFrame()\n",
        "\n",
        "splits = 5\n",
        "cv = None\n",
        "j = 0\n",
        "for i in models:\n",
        "    model = i\n",
        "    cv = KFold(n_splits = splits, random_state = None, shuffle = True)\n",
        "    KFold_Score[classifiers[j]] = np.sqrt(-(cross_val_score(model, train_data, train_label, scoring='neg_mean_squared_error', cv=cv)))\n",
        "    j = j+1\n",
        "mean = pd.DataFrame(KFold_Score.mean(), index= classifiers)\n",
        "\n",
        "folds = []\n",
        "for f in range(1,splits+1):\n",
        "    fold_val = 'Fold ' + str(f)\n",
        "    folds.append(fold_val)\n",
        "folds.append('Mean')\n",
        "\n",
        "KFold_Score = pd.concat([KFold_Score,mean.T])\n",
        "KFold_Score.index = folds\n",
        "KFold_Score = KFold_Score.T.sort_values(by=['Mean'], ascending = True)\n",
        "KFold_Score = KFold_Score.reset_index()\n",
        "KFold_Score.rename(columns={\"index\": \"Regression Model\"}, inplace=True)\n",
        "KFold_Score\n"
      ],
      "metadata": {
        "id": "TljrdendlzKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Test_Score = pd.DataFrame()\n",
        "rmse_values = []\n",
        "\n",
        "for i in range(len(classifiers)):\n",
        "    model = models[i]\n",
        "    model.fit(train_data,train_label)\n",
        "    test_pred = model.predict(test_data)\n",
        "    RMSE = mean_squared_error(test_label,test_pred, squared = False)\n",
        "    rmse_values.append(RMSE)\n",
        "\n",
        "Test_Score[\"Regression Model\"] = classifiers\n",
        "Test_Score[\"RMSE\"] = rmse_values\n",
        "\n",
        "Test_Score = Test_Score.sort_values(by=['RMSE'], ascending = True)\n",
        "Test_Score.reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "jwbC7Z4ml2oM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#randomforest\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, Y)\n",
        "importances = rf.feature_importances_\n",
        "\n",
        "feat_importances = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "feat_importances = feat_importances.sort_values(by='Importance', ascending=False)\n",
        "print(feat_importances)\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.barh(feat_importances['Feature'], feat_importances['Importance'], color='skyblue')\n",
        "plt.xlabel('feature_importance')\n",
        "plt.ylabel('features')\n",
        "plt.title('feature_importance_whit_randomforest')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1-onGiIzl5da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Kmeans_model\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.cross_validation import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from matplotlib.mlab import PCA\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from subprocess import check_output\n",
        "from pandas import read_csv\n",
        "\n",
        "#Normalize\n",
        "def normalizeData(x):\n",
        "    for idx in (\"radius_mean\",\"fractal_dimension_worst\"):\n",
        "        x[idx]=x[idx]-min(x[idx])/(max(x[idx])-min(x[idx]))\n",
        "    return x\n",
        "\n",
        "def performPCA(x):\n",
        "    pca=PCA(n_components=9,whiten=True)\n",
        "    x=pca.fit(x).transform(x)\n",
        "    return x\n",
        "\n",
        "def createTrainTestDataSet(df_x,df_y):\n",
        "   x_train,x_test,y_train,y_test=train_test_split(df_x,df_y,test_size=.2,random_state=4)\n",
        "   return x_train,x_test,y_train,y_test\n",
        "\n",
        "#Kmeans_function\n",
        "\n",
        "def kmeansClustering(x_train,y_train):\n",
        "    model=KMeans(n_clusters=2, init='k-means++', n_init=10, max_iter=300)\n",
        "    fittedModel=model.fit(x_train, y_train)\n",
        "    return fittedModel\n",
        "\n",
        "def envSetup():\n",
        "    print(\"Read data from CSV ::\")\n",
        "    bcData=readDataSet()\n",
        "    print(\"Dataset created successfully!!!\")\n",
        "    print(\"Cleanse dataset ::\")\n",
        "    df_x,df_y=cleanseDataSet(bcData)\n",
        "    print(\"Dataset cleansed successfully!!!\")\n",
        "    print(\"Normalize dataset ::\")\n",
        "    df_x=normalizeData(df_x)\n",
        "    print(\"Dataset Normalized successfully!!!\")\n",
        "    print(\"Find correlation using PCA ::\")\n",
        "    x_train,x_test,y_train,y_test=createTrainTestDataSet(df_x,df_y)\n",
        "    print(\"Dataset divided as 80% train dataset & 20 test dataset\")\n",
        "    return x_train,x_test,y_train,y_test\n"
      ],
      "metadata": {
        "id": "z8rmTfdOl8Uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def choiceKMeans():\n",
        "    print(\"++++++++++++++++++ K_MEANS CLUSTERING ++++++++++++++++++\")\n",
        "    print(\"Perform K_Means Classification ::\")\n",
        "    fittedModel=kmeansClustering(x_train,y_train)\n",
        "    print(\"K-Means train model created successfully!!!\")\n",
        "    print(\"Validate the clustering model ::\")\n",
        "    predictions=getPrediction(fittedModel,x_test)\n",
        "    print(\"Clustering model tested successfully!!!\")\n",
        "    print(\"Get the Confussion Matrix ::\")\n",
        "    confusion_mat=getConfusionMatrix(y_test, predictions)\n",
        "    print(confusion_mat)\n",
        "    print(\"Get the Accuracy ::\")\n",
        "    accuracy=getAccuracy(y_test, predictions)\n",
        "    print(accuracy)\n"
      ],
      "metadata": {
        "id": "3fxCqkZ9mAvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For correlation\n",
        "x_train,x_test,y_train,y_test=envSetup()\n",
        "\n",
        "Read data from CSV ::\n",
        "Dataset created successfully!!!\n",
        "Cleanse dataset ::\n",
        "Dataset cleansed successfully!!!\n",
        "Normalize dataset ::\n",
        "Dataset Normalized successfully!!!\n",
        "Create Train & Test Data set ::\n",
        "Dataset divided as 80% train dataset & 20 test dataset\n",
        "++++++++++++++++++ K_MEANS CLUSTERING ++++++++++++++++++\n",
        "Perform K_Means Classification ::\n",
        "K-Means train model created successfully!!!\n",
        "Validate the clustering model ::\n",
        "Clustering model tested successfully!!!\n"
      ],
      "metadata": {
        "id": "QI0UMV-hmDKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For variance-treshold\n",
        "x_train,x_test,y_train,y_test=envSetup()\n",
        "\n",
        "Read data from CSV ::\n",
        "Dataset created successfully!!!\n",
        "Cleanse dataset ::\n",
        "Dataset cleansed successfully!!!\n",
        "Normalize dataset ::\n",
        "Dataset Normalized successfully!!!\n",
        "Create Train & Test Data set ::\n",
        "Dataset divided as 80% train dataset & 20 test dataset\n",
        "++++++++++++++++++ K_MEANS CLUSTERING ++++++++++++++++++\n",
        "Perform K_Means Classification ::\n",
        "K-Means train model created successfully!!!\n",
        "Validate the clustering model ::\n",
        "Clustering model tested successfully!!!\n"
      ],
      "metadata": {
        "id": "1mRNVJjzmL5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For MI\n",
        "x_train,x_test,y_train,y_test=envSetup()\n",
        "\n",
        "Read data from CSV ::\n",
        "Dataset created successfully!!!\n",
        "Cleanse dataset ::\n",
        "Dataset cleansed successfully!!!\n",
        "Normalize dataset ::\n",
        "Dataset Normalized successfully!!!\n",
        "Create Train & Test Data set ::\n",
        "Dataset divided as 80% train dataset & 20 test dataset\n",
        "++++++++++++++++++ K_MEANS CLUSTERING ++++++++++++++++++\n",
        "Perform K_Means Classification ::\n",
        "K-Means train model created successfully!!!\n",
        "Validate the clustering model ::\n",
        "Clustering model tested successfully!!!\n"
      ],
      "metadata": {
        "id": "iZnQxbXzmO-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Forward-fs\n",
        "x_train,x_test,y_train,y_test=envSetup()\n",
        "\n",
        "Read data from CSV ::\n",
        "Dataset created successfully!!!\n",
        "Cleanse dataset ::\n",
        "Dataset cleansed successfully!!!\n",
        "Normalize dataset ::\n",
        "Dataset Normalized successfully!!!\n",
        "Create Train & Test Data set ::\n",
        "Dataset divided as 80% train dataset & 20 test dataset\n",
        "++++++++++++++++++ K_MEANS CLUSTERING ++++++++++++++++++\n",
        "Perform K_Means Classification ::\n",
        "K-Means train model created successfully!!!\n",
        "Validate the clustering model ::\n",
        "Clustering model tested successfully!!!\n"
      ],
      "metadata": {
        "id": "wNt56C2QmSdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Backward-fs\n",
        "x_train,x_test,y_train,y_test=envSetup()\n",
        "\n",
        "Read data from CSV ::\n",
        "Dataset created successfully!!!\n",
        "Cleanse dataset ::\n",
        "Dataset cleansed successfully!!!\n",
        "Normalize dataset ::\n",
        "Dataset Normalized successfully!!!\n",
        "Create Train & Test Data set ::\n",
        "Dataset divided as 80% train dataset & 20 test dataset\n",
        "++++++++++++++++++ K_MEANS CLUSTERING ++++++++++++++++++\n",
        "Perform K_Means Classification ::\n",
        "K-Means train model created successfully!!!\n",
        "Validate the clustering model ::\n",
        "Clustering model tested successfully!!!\n"
      ],
      "metadata": {
        "id": "N8B8BlI2mVvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For comperhansive-fs\n",
        "x_train,x_test,y_train,y_test=envSetup()\n",
        "\n",
        "Read data from CSV ::\n",
        "Dataset created successfully!!!\n",
        "Cleanse dataset ::\n",
        "Dataset cleansed successfully!!!\n",
        "Normalize dataset ::\n",
        "Dataset Normalized successfully!!!\n",
        "Create Train & Test Data set ::\n",
        "Dataset divided as 80% train dataset & 20 test dataset\n",
        "++++++++++++++++++ K_MEANS CLUSTERING ++++++++++++++++++\n",
        "Perform K_Means Classification ::\n",
        "K-Means train model created successfully!!!\n",
        "Validate the clustering model ::\n",
        "Clustering model tested successfully!!!\n"
      ],
      "metadata": {
        "id": "Gv_Vhlv5mYgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For regularization-fs\n",
        "x_train,x_test,y_train,y_test=envSetup()\n",
        "\n",
        "Read data from CSV ::\n",
        "Dataset created successfully!!!\n",
        "Cleanse dataset ::\n",
        "Dataset cleansed successfully!!!\n",
        "Normalize dataset ::\n",
        "Dataset Normalized successfully!!!\n",
        "Create Train & Test Data set ::\n",
        "Dataset divided as 80% train dataset & 20 test dataset\n",
        "++++++++++++++++++ K_MEANS CLUSTERING ++++++++++++++++++\n",
        "Perform K_Means Classification ::\n",
        "K-Means train model created successfully!!!\n",
        "Validate the clustering model ::\n",
        "Clustering model tested successfully!!!\n"
      ],
      "metadata": {
        "id": "cQpL2lFKmbDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For randomforest-fs\n",
        "x_train,x_test,y_train,y_test=envSetup()\n",
        "\n",
        "Read data from CSV ::\n",
        "Dataset created successfully!!!\n",
        "Cleanse dataset ::\n",
        "Dataset cleansed successfully!!!\n",
        "Normalize dataset ::\n",
        "Dataset Normalized successfully!!!\n",
        "Create Train & Test Data set ::\n",
        "Dataset divided as 80% train dataset & 20 test dataset\n",
        "++++++++++++++++++ K_MEANS CLUSTERING ++++++++++++++++++\n",
        "Perform K_Means Classification ::\n",
        "K-Means train model created successfully!!!\n",
        "Validate the clustering model ::\n",
        "Clustering model tested successfully!!!\n"
      ],
      "metadata": {
        "id": "wQIntfcmmdt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OrfPjSyDmgKw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}